/home/cetec/AIProjects/mueva_test/muvera_test.py:79: FutureWarning: 

All support for the `google.generativeai` package has ended. It will no longer be receiving 
updates or bug fixes. Please switch to the `google.genai` package as soon as possible.
See README for more details:

https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md

  import google.generativeai as genai
üöÄ Iniciando Debug de Recuperaci√≥n con IMAGEN...
‚úÖ LangSmith configurado

================================================================================
üöÄ SISTEMA RAG HISTOPATOLOG√çA - ColPali PURO + MUVERA + LangGraph
================================================================================

üñºÔ∏è Inicializando ColPali Puro + MUVERA...
   üìö Cargando ColPali v1.2 (texto + im√°genes)...
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 28728.11it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.06s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.79s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
   ‚úÖ ColPali cargado (128D multi-vector)
   üöÄ Inicializando MUVERA...
   ‚úÖ MUVERA inicializado (FDE: 20480D)
üñºÔ∏è Usando imagen: uploads/query_image_d2a978a7a73e427db0a93e0f6de4f23e.jpg


üîé CONSULTA MULTIMODAL: 'analiza esta imagen histologica y dime que organo es' + IMAGEN

üì® Recibiendo consulta: analiza esta imagen histologica y dime que organo ...

üîç Ejecutando b√∫squeda en Qdrant...
üîó Cliente Qdrant conectado

üìÑ Resultados recuperados (5):
   [1] Score: 668.5834 | Doc: unknown (Pg ?)
       Imagen: histopatologia_data/embeddings/arch2_page_3.jpg
   [2] Score: 665.8362 | Doc: unknown (Pg ?)
       Imagen: histopatologia_data/embeddings/arch2_page_5.jpg
   [3] Score: 640.3456 | Doc: unknown (Pg ?)
       Imagen: histopatologia_data/embeddings/arch2_page_14.jpg
   [4] Score: 638.4885 | Doc: unknown (Pg ?)
       Imagen: histopatologia_data/embeddings/arch2_page_15.jpg
   [5] Score: 633.8242 | Doc: unknown (Pg ?)
       Imagen: histopatologia_data/embeddings/arch2_page_8.jpg

üí≠ Generando respuesta multimodal...
Traceback (most recent call last):
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py", line 3088, in _agenerate
    await self.client.aio.models.generate_content(
        **request,
    )
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/google/genai/models.py", line 7331, in generate_content
    response = await self._generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        model=model, contents=contents, config=parsed_config
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/google/genai/models.py", line 6095, in _generate_content
    response = await self._api_client.async_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        'post', path, request_dict, http_options
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/google/genai/_api_client.py", line 1432, in async_request
    result = await self._async_request(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
        http_request=http_request, http_options=http_options, stream=False
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/google/genai/_api_client.py", line 1364, in _async_request
    return await retry(self._async_request_once, http_request, stream)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py", line 112, in __call__
    do = await self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py", line 157, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/tenacity/_utils.py", line 111, in inner
    return call(*args, **kwargs)
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/tenacity/__init__.py", line 413, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/tenacity/__init__.py", line 184, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/usr/local/lib/python3.13/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "/usr/local/lib/python3.13/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py", line 116, in __call__
    result = await fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/google/genai/_api_client.py", line 1310, in _async_request_once
    await errors.APIError.raise_for_async_response(response)
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/google/genai/errors.py", line 216, in raise_for_async_response
    await cls.raise_error_async(status_code, response_json, response)
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/google/genai/errors.py", line 238, in raise_error_async
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 55.918181217s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '55s'}]}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cetec/AIProjects/mueva_test/debug_retrieval.py", line 43, in <module>
    asyncio.run(debug_retrieval())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/nest_asyncio.py", line 30, in run
    return loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/nest_asyncio.py", line 98, in run_until_complete
    return f.result()
           ~~~~~~~~^^
  File "/usr/local/lib/python3.13/asyncio/futures.py", line 199, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/usr/local/lib/python3.13/asyncio/tasks.py", line 304, in __step_run_and_handle_result
    result = coro.send(None)
  File "/home/cetec/AIProjects/mueva_test/debug_retrieval.py", line 33, in debug_retrieval
    respuesta = await sistema.procesar_consulta(consulta=consulta, imagen_path=latest_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/muvera_test.py", line 897, in procesar_consulta
    final_state = await self.compiled_graph.ainvoke(initial_state, config=config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py", line 3161, in ainvoke
    async for chunk in self.astream(
    ...<29 lines>...
            chunks.append(chunk)
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py", line 2974, in astream
    async for _ in runner.atick(
    ...<13 lines>...
            yield o
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py", line 304, in atick
    await arun_with_retry(
    ...<15 lines>...
    )
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py", line 138, in arun_with_retry
    return await task.proc.ainvoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py", line 705, in ainvoke
    input = await asyncio.create_task(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^
        step.ainvoke(input, config, **kwargs), context=context
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/usr/local/lib/python3.13/asyncio/futures.py", line 286, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/usr/local/lib/python3.13/asyncio/tasks.py", line 375, in __wakeup
    future.result()
    ~~~~~~~~~~~~~^^
  File "/usr/local/lib/python3.13/asyncio/futures.py", line 199, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/usr/local/lib/python3.13/asyncio/tasks.py", line 304, in __step_run_and_handle_result
    result = coro.send(None)
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py", line 473, in ainvoke
    ret = await self.afunc(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/muvera_test.py", line 815, in _nodo_generar_respuesta
    response = await self.llm.ainvoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 425, in ainvoke
    llm_result = await self.agenerate_prompt(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<8 lines>...
    )
    ^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 1132, in agenerate_prompt
    return await self.agenerate(
           ^^^^^^^^^^^^^^^^^^^^^
        prompt_messages, stop=stop, callbacks=callbacks, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 1090, in agenerate
    raise exceptions[0]
  File "/usr/local/lib/python3.13/asyncio/tasks.py", line 304, in __step_run_and_handle_result
    result = coro.send(None)
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 1359, in _agenerate_with_cache
    result = await self._agenerate(
             ^^^^^^^^^^^^^^^^^^^^^^
        messages, stop=stop, run_manager=run_manager, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py", line 3093, in _agenerate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "/home/cetec/AIProjects/mueva_test/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 55.918181217s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '55s'}]}}
During task with name 'generar_respuesta' and id '8c2a94f7-28cf-d828-6f49-48b702236b0d'
